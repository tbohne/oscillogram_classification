{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tslearn.preprocessing import TimeSeriesResampler\n",
    "import random\n",
    "\n",
    "from oscillogram_classification.cam import gen_heatmap_dictionary, plot_heatmaps_as_overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02464d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_measurement(filename: str) -> pandas.DataFrame:\n",
    "    df = pandas.read_csv(filename, delimiter=\";\", na_values=[\"-∞\", \"∞\"])\n",
    "    df = df[1:].apply(lambda x: x.str.replace(\",\", \".\"))\n",
    "    df = df.astype(float).dropna()\n",
    "    return df\n",
    "\n",
    "def z_normalize_time_series(series):\n",
    "    return (series - np.mean(series)) / np.std(series)\n",
    "\n",
    "def plot_signals_with_channels(signals, colors, channel_titles, signal_titles, figsize):\n",
    "    fig, axs = plt.subplots(len(signals), len(colors), figsize=figsize)\n",
    "    for signal_idx, signal in enumerate(signals):\n",
    "        for channel_idx, channel in enumerate(signal):\n",
    "            axs[signal_idx, channel_idx].plot(channel, color=colors[channel_idx])\n",
    "            if signal_idx == 0:\n",
    "                axs[signal_idx, channel_idx].set_title(channel_titles[channel_idx])\n",
    "            if channel_idx == 0:\n",
    "                axs[signal_idx, channel_idx].set_ylabel(signal_titles[signal_idx])\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"data_vis.svg\", format=\"svg\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def resample(signals: np.ndarray, znorm: bool) -> np.ndarray:\n",
    "    target_len = 500 # int(np.average([len(chan) for signal in signals for chan in signal ]))\n",
    "    print(\"target len\", target_len)\n",
    "    for i in range(len(signals)):\n",
    "        for j in range(len(signals[i])):\n",
    "            sig_arr = np.array(signals[i][j])\n",
    "            sig_arr = sig_arr.reshape((1, len(signals[i][j]), 1))  # n_ts, sz, d\n",
    "            signals[i][j] = TimeSeriesResampler(sz=target_len).fit_transform(sig_arr).tolist()[0]\n",
    "            \n",
    "            # z-normalization\n",
    "            if znorm:\n",
    "                signals[i][j] = z_normalize_time_series(signals[i][j])\n",
    "            \n",
    "    return np.array(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset_from_csv(csv_files, dir_path):\n",
    "    signals = []\n",
    "    labels = []\n",
    "    signal_titles = []\n",
    "    time_values = []\n",
    "    for sample in csv_files:\n",
    "        sig = load_measurement(dir_path + sample)\n",
    "        signals.append([sig[channel_name] for channel_name in sig.columns.tolist() if not channel_name == \"Zeit\"])\n",
    "        signal_titles.append(str(len(signals)) + \"_\" + sample)\n",
    "        time_values.append([sig[\"Zeit\"]])\n",
    "        # 0 -> neg, 1 -> pos\n",
    "        if \"POS\" in sample:\n",
    "            labels.append(1)\n",
    "        elif \"NEG\" in sample:\n",
    "            labels.append(0)\n",
    "    return signals, labels, signal_titles, time_values\n",
    "\n",
    "import os\n",
    "train_data = \"../data/samples/Messwoche_06112023-10112023/train/\"\n",
    "test_data = \"../data/samples/Messwoche_06112023-10112023/test/\"\n",
    "train_csv_files = [f for f in os.listdir(train_data) if f.endswith('.csv')]\n",
    "test_csv_files = [f for f in os.listdir(test_data) if f.endswith('.csv')]\n",
    "\n",
    "train_signals, train_labels, train_titles, train_time_values = gen_dataset_from_csv(train_csv_files, train_data)\n",
    "test_signals, test_labels, test_titles, test_time_values = gen_dataset_from_csv(test_csv_files, test_data)\n",
    "\n",
    "print(\"pos train:\", train_labels.count(1), \"neg train:\", train_labels.count(0), \"prop:\", train_labels.count(1) / train_labels.count(0))\n",
    "print(\"pos test:\", test_labels.count(1), \"neg test:\", test_labels.count(0), \"prop:\", test_labels.count(1) / test_labels.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc132db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling\n",
    "\n",
    "# True -> znorm\n",
    "train_signals = resample(train_signals, True)\n",
    "test_signals = resample(test_signals, True)\n",
    "train_time_values = resample(train_time_values, True)\n",
    "test_time_values = resample(test_time_values, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faaf30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_titles = [\"Kanal A - Heizung (Masse)\", \"Kanal B - Heizung (Plus)\", \"Kanal C - Signal (+)\", \"Kanal D - Signal (-)\", \n",
    "                  \"Kanal E - 5V\", \"Kanal F - Temperatur Signal\", \"Kanal G - Masse\", \"Kanal H - Druck Signal\"] if \\\n",
    "    \"Messwoche_KW16\" in train_data else ['Lambdasonde', 'Luftmassenmesser', 'Differenzdrucksensor', 'Abgastemperatur', 'Nockenwellendrehzahlsensor']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#fff700', '#00fbff', '#ff68d1']\n",
    "colors = colors[:len(channel_titles)]\n",
    "plot_signals_with_channels(train_signals, colors, channel_titles, train_titles, figsize=(20, 3 * len(train_signals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0207cb",
   "metadata": {},
   "source": [
    "## Training with z-normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915737ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "# deactivate tensorflow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"before:\", train_signals.shape)\n",
    "print(\"before:\", test_signals.shape)\n",
    "\n",
    "num_samples = train_signals.shape[0]\n",
    "sample_len = train_signals.shape[2]\n",
    "num_chan = train_signals.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164924d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate - using all channels\n",
    "\n",
    "# shape[0] samples, sample length shape[2], shape[1] channels\n",
    "train_signals = train_signals.reshape(train_signals.shape[0], train_signals.shape[2], train_signals.shape[1])\n",
    "test_signals = test_signals.reshape(test_signals.shape[0], test_signals.shape[2], test_signals.shape[1])\n",
    "\n",
    "assert len(train_signals) == num_samples\n",
    "assert len(train_signals[0]) == sample_len\n",
    "assert len(train_signals[0][0]) == num_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4325b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate - using only \"Lambdasonde\"\n",
    "train_signals = train_signals[:, 0, :]\n",
    "test_signals = test_signals[:, 0, :]\n",
    "\n",
    "print(train_signals.shape)\n",
    "plt.plot(train_signals[0])\n",
    "plt.title(\"Lambdasonde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac78d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate - using only \"Luftmassenmesser\"\n",
    "train_signals = train_signals[:, 1, :]\n",
    "test_signals = test_signals[:, 1, :]\n",
    "\n",
    "print(train_signals.shape)\n",
    "plt.plot(train_signals[0])\n",
    "plt.title(\"Luftmassenmesser\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f18c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate - using only \"Differenzdrucksensor\"\n",
    "train_signals = train_signals[:, 2, :]\n",
    "test_signals = test_signals[:, 2, :]\n",
    "\n",
    "print(train_signals.shape)\n",
    "plt.plot(train_signals[0])\n",
    "plt.title(\"Differenzdrucksensor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate - using only \"Abgastemperatur\"\n",
    "train_signals = train_signals[:, 3, :]\n",
    "test_signals = test_signals[:, 3, :]\n",
    "\n",
    "print(train_signals.shape)\n",
    "plt.plot(train_signals[0])\n",
    "plt.title(\"Abgastemperatur\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate - using only \"Nockenwellendrehzahlsensor\"\n",
    "train_signals = train_signals[:, 4, :]\n",
    "test_signals = test_signals[:, 4, :]\n",
    "\n",
    "print(train_signals.shape)\n",
    "plt.plot(train_signals[0])\n",
    "plt.title(\"Nockenwellendrehzahlsensor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"after:\", train_signals.shape)\n",
    "print(\"after:\", test_signals.shape)\n",
    "\n",
    "num_classes = len(np.unique(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ebb99",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "- FCN\n",
    "- hyperparameters (`kernel_size, filters, usage of BatchNorm`) found using `KerasTuner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb333aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = build_model(input_shape=train_signals.shape[1:])\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babd622-1f7e-4a62-9c13-37e84159d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oscillogram_classification import models\n",
    "\n",
    "model = models.create_resnet_model(input_shape=train_signals.shape[1:],num_classes=2)\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "# there should be no model, otherwise retraining!\n",
    "assert not os.path.isfile(\"best_model.keras\")\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1)\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_signals,\n",
    "    train_labels,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval model on test data\n",
    "\n",
    "model = keras.models.load_model(\"best_model.keras\")\n",
    "test_loss, test_acc = model.evaluate(test_signals, test_labels)\n",
    "\n",
    "print(\"test acc.:\", test_acc)\n",
    "print(\"test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a629eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation loss\n",
    "\n",
    "metric = \"sparse_categorical_accuracy\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"model \" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2543a3-bc2b-480e-8e31-620eb10aa33e",
   "metadata": {},
   "source": [
    "### Grad-CAM on univariate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa7e13-8034-4027-93a7-81976e38d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"tf-keras-gradcam++\"\n",
    "\n",
    "random_index = random.randint(0, len(test_signals)-1)\n",
    "net_input = test_signals[random_index]\n",
    "assert net_input.shape[1] == 1\n",
    "ground_truth = test_labels[random_index]\n",
    "prediction = model.predict(np.array([net_input]))\n",
    "heatmaps = gen_heatmap_dictionary(method, np.array(net_input), model, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab7333-bc09-40d6-9a0d-006f7e0317fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps_as_overlay(heatmaps, net_input, 'test_plot', test_time_values.squeeze()[random_index].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e11f6f",
   "metadata": {},
   "source": [
    "## tsai Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ad8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.all import *\n",
    "import sklearn.metrics as skm\n",
    "my_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9eef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tsai_dataset(train_signals, train_labels):\n",
    "\n",
    "    # randomly split the indices of the training samples into two sets (train (80%) and validation (20%))\n",
    "    # 'splits' contains a tuple of lists ([train_indices], [validation_indices])\n",
    "    #    - stratify=True -> split the data in such a way that each class's proportion in the train and validation\n",
    "    #      datasets is approximately the same as the proportion in the original dataset\n",
    "    #    - random_state is the seed\n",
    "    splits = get_splits(train_labels, valid_size=.2, stratify=True, random_state=23, shuffle=True)\n",
    "    print(splits)\n",
    "    print(\"--> currently, the above plot wrongly labels 'Valid' as 'Test'\")\n",
    "\n",
    "    # define transformations:\n",
    "    #    - None -> no transformation to the input (X)\n",
    "    #    - Categorize() -> convert labels into categorical format; converts the labels to integers\n",
    "    # my labels are already ints, but I'll leave it here as a more general case\n",
    "    tfms  = [None, [Categorize()]]\n",
    "    \n",
    "    # creates tensors to train on, e.g., \n",
    "    #     dsets[0]: (TSTensor(vars:5, len:500, device=cpu, dtype=torch.float32), TensorCategory(0))\n",
    "    dsets = TSDatasets(train_signals, train_labels, tfms=tfms, splits=splits, inplace=True)\n",
    "    \n",
    "    print(\"#train samples:\", len(dsets.train))\n",
    "    print(\"#valid samples:\", len(dsets.valid))\n",
    "    \n",
    "    # data loaders: loading data in batches; batch size 64 for training and 128 for validation\n",
    "    #    - TSStandardize: batch normalization\n",
    "    #    - num_workers: 0 -> data loaded in main process\n",
    "    dls = TSDataLoaders.from_dsets(\n",
    "        dsets.train, dsets.valid, bs=[64, 128], batch_tfms=[TSStandardize()], num_workers=0\n",
    "    )\n",
    "    \n",
    "    # vis a batch\n",
    "    dls.show_batch(nrows=3, ncols=3, sharey=True)\n",
    "\n",
    "    return dls\n",
    "\n",
    "def train_tsai_model():\n",
    "    # learner encapsulates the data, the model, and other details related to the training process\n",
    "    learn = Learner(dls, model, metrics=accuracy)\n",
    "    \n",
    "    # saves curr state of learner (model + weights) to a file named stage0\n",
    "    learn.save('stage0')\n",
    "    \n",
    "    # load state of model\n",
    "    learn.load('stage0')\n",
    "    \n",
    "    # training over range of learning rates -- find suitable LR (or LR range)\n",
    "    #   - learning rate range where the loss decreases most effectively\n",
    "    learn.lr_find()\n",
    "    \n",
    "    # 150 -> num of epochs\n",
    "    #    - involves varying the learning rate in a specific way during training\n",
    "    #    - the cyclical nature helps in faster convergence, avoids getting stuck in local minima,\n",
    "    #      and sometimes achieves better overall performance\n",
    "    #    - it provides a balance between exploring the loss landscape (with higher learning rates)\n",
    "    #    - and exploiting known good areas of the landscape (with lower learning rates)\n",
    "    learn.fit_one_cycle(150, lr_max=1e-3)\n",
    "    \n",
    "    learn.save('stage1')\n",
    "    return learn\n",
    "\n",
    "# labeled test data\n",
    "\n",
    "def test_tsai_model(test_signals, test_labels):\n",
    "\n",
    "    test_ds = TSDatasets(test_signals, test_labels, tfms=[None, [Categorize()]])\n",
    "    test_dl = dls.valid.new(test_ds)\n",
    "    \n",
    "    test_probas, test_targets, test_preds = learn.get_preds(\n",
    "        dl=test_dl, with_decoded=True, save_preds=None, save_targs=None\n",
    "    )\n",
    "\n",
    "    return skm.accuracy_score(test_targets, test_preds)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053502a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsai expects the data in a diff format: (samples, variables, length)\n",
    "\n",
    "# variables = 1 for univariate datasets and >1 for multivariate\n",
    "\n",
    "train_signals = train_signals.reshape(train_signals.shape[0], train_signals.shape[2], train_signals.shape[1])\n",
    "test_signals = test_signals.reshape(test_signals.shape[0], test_signals.shape[2], test_signals.shape[1])\n",
    "\n",
    "print(train_signals.shape)\n",
    "print(test_signals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e58b60-6044-400d-b60e-a024a016e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = generate_tsai_dataset(train_signals, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c68aba7",
   "metadata": {},
   "source": [
    "## Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### models trained on normalized version of RAW TS ###\n",
    "######################################################\n",
    "\n",
    "# creating InceptionTime (is a CNN) model (vars: 5 (5 channels), c: 2 (2 classes))\n",
    "# model = InceptionTime(dls.vars, dls.c)\n",
    "\n",
    "# TODO: InceptionTimePlus\n",
    "\n",
    "# TODO: XceptionTime\n",
    "\n",
    "# TODO: XceptionTimePlus\n",
    "\n",
    "# TODO: OmniScaleCNN\n",
    "\n",
    "# TODO: XCM\n",
    "\n",
    "# creating XCMPlus\n",
    "model = XCMPlus(dls.vars, dls.c, dls.len)\n",
    "\n",
    "# create FCN (CNN model)\n",
    "# model = FCN(dls.vars, dls.c)\n",
    "\n",
    "# TODO: FCNPlus\n",
    "\n",
    "# creating ResNet (CNN)\n",
    "# model = ResNet(dls.vars, dls.c)\n",
    "\n",
    "# TODO: ResNetPlus\n",
    "\n",
    "# TODO: XResNet1d\n",
    "\n",
    "# TODO: XResNet1dPlus\n",
    "\n",
    "# TODO: ResCNN\n",
    "\n",
    "# TODO: TCN\n",
    "\n",
    "# creating RNN\n",
    "# model = RNN(dls.vars, dls.c)\n",
    "\n",
    "# creating RNNPlus (RNN model + including a feature extractor to the RNN network)\n",
    "# model = RNNPlus(dls.vars, dls.c)\n",
    "\n",
    "# TODO: RNNAttention\n",
    "\n",
    "# creating GRU (RNN model)\n",
    "# model = GRU(dls.vars, dls.c)\n",
    "\n",
    "# creating GRUPlus (RNN model + including a feature extractor to the RNN network)\n",
    "# model = GRUPlus(dls.vars, dls.c)\n",
    "\n",
    "# creating GRUAttention (RNN model + attention)\n",
    "#model = GRUAttention(dls.vars, dls.c, seq_len=500)\n",
    "\n",
    "# creating LSTM (RNN model)\n",
    "# model = LSTM(dls.vars, dls.c)\n",
    "\n",
    "# creating LSTMPlus (RNN model + including a feature extractor to the RNN network)\n",
    "# model = LSTMPlus(dls.vars, dls.c)\n",
    "\n",
    "# creating LSTMAttention (RNN model + attention)\n",
    "# model = LSTMAttention(dls.vars, dls.c, seq_len=500)\n",
    "\n",
    "# model = TSSequencerPlus(dls.vars, dls.c, seq_len=500)\n",
    "\n",
    "# model = TransformerModel(dls.vars, dls.c)\n",
    "\n",
    "# TODO: TST\n",
    "\n",
    "# TODO: TSTPlus\n",
    "\n",
    "# TODO: TSPerceiver\n",
    "\n",
    "# TODO: TSiT\n",
    "\n",
    "# TODO: PatchTST\n",
    "\n",
    "# TODO: ROCKETs category\n",
    "\n",
    "# TODO: Wavelet-based NNs category\n",
    "\n",
    "# TODO: Hybrid models category\n",
    "\n",
    "# TODO: Tabular models category\n",
    "\n",
    "#########################################\n",
    "### models trained on feature vectors ###\n",
    "#########################################\n",
    "\n",
    "# TODO: extract + select features, i.e., generate feature vectors\n",
    "\n",
    "# TODO: MLP\n",
    "\n",
    "# TODO: gMLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906357a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d21e7c-9f99-44ef-b2e9-a951b0089f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = train_tsai_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab522be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses -> loss development over all epochs for 'train' and 'valid'\n",
    "# final losses ->  zoomed-in view of the final epochs, focusing on loss values towards the end of training\n",
    "# accuracy -> validation accuracy of the model\n",
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_all(path='export', dls_fname='dls', model_fname='model', learner_fname='learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babec318",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(nrows=3, ncols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d37d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_probas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0136f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfab24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intested in cases where the model made incorrect predictions at least 3 times\n",
    "confusions = interp.most_confused(min_val=3)\n",
    "for actual_class, pred_class, times in confusions:\n",
    "    print(\"pred:\", pred_class)\n",
    "    print(\"actual:\", actual_class)\n",
    "    print(times, \"times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824bf6c",
   "metadata": {},
   "source": [
    "## Inference on additional test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06127761-c992-41ff-8d62-73bdf4c6335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test_tsai_model(test_signals, test_labels)\n",
    "print(\"test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814cb52c-272c-4caa-9b9a-e2b7edf1d63e",
   "metadata": {},
   "source": [
    "## gradcam for XCMPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3a358-d65d-4ab3-b9e3-e783d7d4e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(model) == tsai.models.XCMPlus.XCMPlus\n",
    "\n",
    "xb, yb = dls.one_batch()\n",
    "model.show_gradcam(xb[0], yb[0], figsize=(12,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d34b9-abdf-40fd-98a2-273892871e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the built-in gradcam method creates plots that are sometimes unreadable, it is better to visualize it with the\n",
    "# methods from oscillogram_classification.cam\n",
    "\n",
    "input, probabilities, targets, predictions = learn.get_X_preds(xb, yb, with_input = True)\n",
    "predictions = predictions.strip('][').split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b9484-5737-4602-9838-57ebe1f9b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(xb)-1)\n",
    "\n",
    "att_maps = get_attribution_map(model, [model.backbone.conv2dblock, model.backbone.conv1dblock], xb[random_index], detach=True, apply_relu=True)\n",
    "att_maps[0] = (att_maps[0] - att_maps[0].min()) / (att_maps[0].max() - att_maps[0].min())\n",
    "att_maps[1] = (att_maps[1] - att_maps[1].min()) / (att_maps[1].max() - att_maps[1].min())\n",
    "\n",
    "print(\"Ground truth: \", int(yb[random_index]), \" Prediction: \", predictions[random_index])\n",
    "\n",
    "for i in range(input.vars):\n",
    "    plot_heatmaps_as_overlay({\"Variables attribution map\":att_maps[0].numpy()[i]},  xb[random_index,i].numpy(), 'test_plot', range(len(xb[random_index,i])))\n",
    "    plot_heatmaps_as_overlay({\"Time attribution map\":att_maps[1].numpy()[i]},  xb[random_index,i].numpy(), 'test_plot', range(len(xb[random_index,i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e19c0-4da5-4908-bae2-2b3c90d0ef76",
   "metadata": {},
   "source": [
    "## Cross-validation for tsai training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79211e03-84cb-4a96-8800-2d1675cb535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "train_test_splits = get_splits(np.concatenate((train_labels, test_labels), axis=0), n_splits = k, valid_size=.2,  stratify=True, random_state=23, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1bc30a-3575-42de-8860-66519dc78bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_signals = np.concatenate((train_signals, test_signals), axis = 0)\n",
    "all_labels = np.concatenate((train_labels, test_labels), axis = 0)\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "for train_test_split in train_test_splits:\n",
    "\n",
    "    train_split_signals = all_signals[train_test_split[0]]\n",
    "    test_split_signals = all_signals[train_test_split[1]]\n",
    "    train_split_labels = all_labels[train_test_split[0]]\n",
    "    test_split_labels = all_labels[train_test_split[1]]\n",
    "\n",
    "    # the training data will be further split into train and validation\n",
    "    dls = generate_tsai_dataset(train_split_signals, train_split_labels)\n",
    "\n",
    "    model = ResNet(dls.vars, dls.c)\n",
    "    learn = train_tsai_model()\n",
    "    learn.save_all(path='export', dls_fname='dls', model_fname='model', learner_fname='learner')\n",
    "    test_acc = test_tsai_model(test_split_signals, test_split_labels)\n",
    "    test_accuracies.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7605d5-8075-437b-9667-c7d165debde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_accuracies)\n",
    "print(\"Mean accuracy over all folds: \", np.mean(test_accuracies)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc546f2-10d2-4b00-8213-595219b0016e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
